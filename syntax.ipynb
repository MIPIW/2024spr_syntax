{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5f6e652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coreference model import and inference\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tibert import BertForCoreferenceResolution, predict_coref\n",
    "from tibert.utils import pprint_coreference_document\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cbec9d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessFile():\n",
    "    def __init__(self):\n",
    "        self.sentence_type = [\"subject-m-object\", \"subject-e-subject\", \"subject-e-object\"]\n",
    "        \n",
    "        self.command_type = [\"OC-OP\", \"OC-XP\", \"XC-OP\", \"XC-XP\"]\n",
    "\n",
    "        default_path = str(Path(\"./\").resolve()) + os.sep + \"syntax_dataset_revised/\"\n",
    "        \n",
    "        self.tokenx_types = [\"a\", \"the\", \"john\", \"he\", \"every\", \"some\"]\n",
    "        self.tokeny_types = [\"a\", \"the\", \"john\", \"he\"]\n",
    "        self.embed_types = [\"RY\", \"EY\"]\n",
    "\n",
    "        self.sent_combination = [f\"RX-{etype}-{xtype}-{ytype}.txt\" for etype in self.embed_types for xtype in self.tokenx_types for ytype in self.tokeny_types]\n",
    "\n",
    "    def set_files(self, df1, df2, df3):\n",
    "        self.df1 = df1\n",
    "        self.df2 = df2\n",
    "        self.df3 = df3\n",
    "\n",
    "    def three_at_once(self, func):\n",
    "        return func(self.df1), func(self.df2), func(self.df3)\n",
    "\n",
    "\n",
    "class Processor():\n",
    "    def __init__(self, base,  file_types, model, tokenizer):\n",
    "        self.base = base\n",
    "        self.file_types = file_types\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        pass\n",
    "    \n",
    "    def process_each_sent_combination_in_a_type(self, sentence_type, embed_type):\n",
    "\n",
    "        lst = []\n",
    "        folder_url = self.base + sentence_type + os.sep + embed_type\n",
    "        for combination in self.file_types:\n",
    "            combination_text = \"-\".join(combination.split(\".\")[0].split(\"-\")[-3:])\n",
    "            file_url = self.base + sentence_type + os.sep + embed_type + os.sep + \"OC-OP-\" + combination\n",
    "\n",
    "            with open(file_url, \"r\") as f: \n",
    "                content = [i.strip().split(\"\\t\") for i in f.readlines()]\n",
    "            text = [i[0] for i in content]\n",
    "            tuples = [(i[1], i[2]) for i in content]\n",
    "                \n",
    "            coref_out = predict_coref(text, model = self.model ,tokenizer = self.tokenizer, batch_size = 128)\n",
    "            coref_tokens = []\n",
    "            for i in range(len(coref_out)):\n",
    "                coref_tokens.append([[\" \".join(i.tokens) for i in coref_out[i].coref_chains[j]] for j in range(len(coref_out[i].coref_chains))])\n",
    "\n",
    "            really_corefed = []\n",
    "            for idx, x in enumerate(zip(coref_tokens, tuples)):\n",
    "                the_corefs, the_tuple = x\n",
    "                really_corefed.append(any([all([the_token in sub_corefs for the_token in the_tuple]) for sub_corefs in the_corefs]))\n",
    "\n",
    "            lst.append((sentence_type, embed_type, combination_text, round(sum(really_corefed) / len(really_corefed), 4)))\n",
    "\n",
    "        return lst\n",
    "\n",
    "\n",
    "def main(base, model, tokenizer):\n",
    "    \n",
    "    files = ProcessFile()\n",
    "    processor = Processor(base, files.sent_combination, model, tokenizer)\n",
    "    # result = {\"subject-m-object\": {}, \"subject-e-subject\": {}, \"subject-e-object\": {}}\n",
    "    \n",
    "    # result['subject-m-object']['OC-OP'] = processor.process_each_sent_combination_in_a_type(\"subject-m-object\", \"OC-OP\")\n",
    "    # result['subject-m-object']['OC-XP'] = processor.process_each_sent_combination_in_a_type(\"subject-m-object\", \"OC-XP\")\n",
    "    # result['subject-m-object']['XC-OP'] = processor.process_each_sent_combination_in_a_type(\"subject-m-object\", \"XC-OP\")\n",
    "    # result['subject-m-object']['XC-XP'] = processor.process_each_sent_combination_in_a_type(\"subject-m-object\", \"XC-XP\")\n",
    "\n",
    "    # result['subject-e-object']['OC-OP'] = processor.process_each_sent_combination_in_a_type(\"subject-e-object\", \"OC-OP\")\n",
    "    # result['subject-e-object']['OC-XP'] = processor.process_each_sent_combination_in_a_type(\"subject-e-object\", \"OC-XP\")\n",
    "    # result['subject-e-object']['XC-OP'] = processor.process_each_sent_combination_in_a_type(\"subject-e-object\", \"XC-OP\")\n",
    "    # result['subject-e-object']['XC-XP'] = processor.process_each_sent_combination_in_a_type(\"subject-e-object\", \"XC-XP\")\n",
    "\n",
    "    # result['subject-e-subject']['OC-OP'] = processor.process_each_sent_combination_in_a_type(\"subject-e-subject\", \"OC-OP\")\n",
    "    # result['subject-e-subject']['OC-XP'] = processor.process_each_sent_combination_in_a_type(\"subject-e-subject\", \"OC-XP\")\n",
    "    # result['subject-e-subject']['XC-OP'] = processor.process_each_sent_combination_in_a_type(\"subject-e-subject\", \"XC-OP\")\n",
    "    # result['subject-e-subject']['XC-XP'] = processor.process_each_sent_combination_in_a_type(\"subject-e-subject\", \"XC-XP\")\n",
    "\n",
    "    \n",
    "    # with open(\"/home/hyohyeongjang/syntax_finalterm/data.pickle\", \"wb\") as f:\n",
    "    #     pickle.dump(result, f)\n",
    "\n",
    "\n",
    "    #######################################################################\n",
    "\n",
    "    with open(\"/home/hyohyeongjang/syntax_finalterm/data.pickle\", \"rb\") as f:\n",
    "        result = pickle.load(f)\n",
    "\n",
    "    import pandas as pd\n",
    "    for i in [\"subject-m-object\", \"subject-e-subject\", \"subject-e-object\"]:\n",
    "        x1 = pd.DataFrame(result[i]['OC-OP'])[[1,2,3]]\n",
    "        x2 = pd.DataFrame(result[i]['OC-XP'])[[1,2,3]]\n",
    "        x3 = pd.DataFrame(result[i]['XC-OP'])[[1,2,3]]\n",
    "        x4 = pd.DataFrame(result[i]['XC-XP'])[[1,2,3]]\n",
    "\n",
    "        x = pd.concat([x1,x2,x3,x4], axis = 0).groupby([1,2]).mean().unstack().T\n",
    "        x.to_csv(i+\".csv\", index = True)\n",
    "\n",
    "\n",
    "\n",
    "    base = \"/home/hyohyeongjang/syntax_finalterm/\"\n",
    "\n",
    "    smo = pd.read_csv(f\"{base}subject-m-object.csv\").drop(columns=\"Unnamed: 0\")\n",
    "    col = pd.DataFrame(smo['2'].map(lambda x: x.split(\"-\")).values.tolist())\n",
    "    col.columns = ['embeeding', \"subj_quant\", \"obj_quant\"]\n",
    "    smo = pd.concat([col, smo], axis = 1).drop(columns = \"2\")\n",
    "\n",
    "    ses = pd.read_csv(f\"{base}subject-e-subject.csv\").drop(columns=\"Unnamed: 0\")\n",
    "    col = pd.DataFrame(ses['2'].map(lambda x: x.split(\"-\")).values.tolist())\n",
    "    col.columns = ['embeeding', \"subj_quant\", \"obj_quant\"]\n",
    "    ses = pd.concat([col, ses], axis = 1).drop(columns = \"2\")\n",
    "\n",
    "    seo = pd.read_csv(f\"{base}subject-e-object.csv\").drop(columns=\"Unnamed: 0\")\n",
    "    col = pd.DataFrame(seo['2'].map(lambda x: x.split(\"-\")).values.tolist())\n",
    "    col.columns = ['embeeding', \"subj_quant\", \"obj_quant\"]\n",
    "    seo = pd.concat([col, seo], axis = 1).drop(columns = \"2\")\n",
    "\n",
    "\n",
    "    # smo = smo.loc[(smo[files.command_type] > 0.5).apply(lambda x: all(x), axis = 1)]\n",
    "    # ses = ses.loc[(ses[files.command_type] > 0.5).apply(lambda x: all(x), axis = 1)]\n",
    "    # seo = seo.loc[(seo[files.command_type] > 0.5).apply(lambda x: all(x), axis = 1)]\n",
    "    def clip_exp_exp(df):\n",
    "        return df.loc[df['subj_quant'].map(lambda x: x in ['john', 'he']) & df['obj_quant'].map(lambda x: x in ['john', 'he'])].sort_values(by = [\"subj_quant\", \"obj_quant\", 'embeeding'])    \n",
    "    \n",
    "    def clip_quant_exp(df):\n",
    "        return df.loc[df['subj_quant'].map(lambda x: x in ['a', 'the', 'every', 'some']) & df['obj_quant'].map(lambda x: x in ['he', 'john'])].sort_values(by = [\"subj_quant\", \"obj_quant\", 'embeeding'])    \n",
    "    \n",
    "    def clip_exp_quant(df):\n",
    "        return df.loc[df['subj_quant'].map(lambda x: x in ['he', 'john']) & df['obj_quant'].map(lambda x: x in ['a', 'the'])].sort_values(by = [\"subj_quant\", \"obj_quant\", 'embeeding'])\n",
    "\n",
    "    def clip_quant_quant(df):\n",
    "        return df.loc[df['subj_quant'].map(lambda x: x in ['a', 'the','every','some']) & df['obj_quant'].map(lambda x: x in ['a', 'the'])].sort_values(by = [\"subj_quant\", \"obj_quant\", 'embeeding'])    \n",
    "    \n",
    "\n",
    "    files.set_files(smo, ses, seo)\n",
    "    return (files.three_at_once(clip_exp_exp),\n",
    "           files.three_at_once(clip_quant_exp),\n",
    "           files.three_at_once(clip_exp_quant),\n",
    "           files.three_at_once(clip_quant_quant))\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "23eb408b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/hyohyeongjang/.cache/huggingface/hub/models--compnet-renard--bert-base-cased-literary-coref/snapshots/5d70bdeda174f8e3dde073b2d5c0f528611ca135/config.json\n",
      "Model config BertForCoreferenceResolutionConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"antecedents_nb\": 350,\n",
      "  \"architectures\": [\n",
      "    \"BertForCoreferenceResolution\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.3,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.3,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_span_size\": 7,\n",
      "  \"mention_loss_coeff\": 0.1,\n",
      "  \"mention_scorer_dropout\": 0.1,\n",
      "  \"mention_scorer_hidden_size\": 3000,\n",
      "  \"mentions_per_token\": 0.4,\n",
      "  \"mentions_per_tokens\": 0.4,\n",
      "  \"metadatas_features_size\": 20,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"segment_size\": 128,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/hyohyeongjang/.cache/huggingface/hub/models--compnet-renard--bert-base-cased-literary-coref/snapshots/5d70bdeda174f8e3dde073b2d5c0f528611ca135/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForCoreferenceResolution.\n",
      "\n",
      "All the weights of BertForCoreferenceResolution were initialized from the model checkpoint at compnet-renard/bert-base-cased-literary-coref.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForCoreferenceResolution for predictions without further training.\n",
      "loading file vocab.txt from cache at /home/hyohyeongjang/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/hyohyeongjang/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/hyohyeongjang/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/hyohyeongjang/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 846 ms, sys: 422 ms, total: 1.27 s\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base = \"/home/hyohyeongjang/syntax_finalterm/syntax_dataset_revised/\"\n",
    "    model_checkpoint = \"compnet-renard/bert-base-cased-literary-coref\"\n",
    "    coref_model = BertForCoreferenceResolution.from_pretrained(model_checkpoint)\n",
    "    tokenizer_checkpoint = \"bert-base-cased\"\n",
    "    coref_tokenizer = BertTokenizerFast.from_pretrained(tokenizer_checkpoint)\n",
    "\n",
    "    \n",
    "    exp_exp, quant_exp, exp_quant, quant_quant = main(base, coref_model, coref_tokenizer)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3029d3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embeeding</th>\n",
       "      <th>subj_quant</th>\n",
       "      <th>obj_quant</th>\n",
       "      <th>OC-OP</th>\n",
       "      <th>OC-XP</th>\n",
       "      <th>XC-OP</th>\n",
       "      <th>XC-XP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EY</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RY</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EY</td>\n",
       "      <td>a</td>\n",
       "      <td>the</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RY</td>\n",
       "      <td>a</td>\n",
       "      <td>the</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EY</td>\n",
       "      <td>every</td>\n",
       "      <td>a</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>RY</td>\n",
       "      <td>every</td>\n",
       "      <td>a</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EY</td>\n",
       "      <td>every</td>\n",
       "      <td>the</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>RY</td>\n",
       "      <td>every</td>\n",
       "      <td>the</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>EY</td>\n",
       "      <td>some</td>\n",
       "      <td>a</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>RY</td>\n",
       "      <td>some</td>\n",
       "      <td>a</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>EY</td>\n",
       "      <td>some</td>\n",
       "      <td>the</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>RY</td>\n",
       "      <td>some</td>\n",
       "      <td>the</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>EY</td>\n",
       "      <td>the</td>\n",
       "      <td>a</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>RY</td>\n",
       "      <td>the</td>\n",
       "      <td>a</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>EY</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>RY</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   embeeding subj_quant obj_quant  OC-OP  OC-XP  XC-OP  XC-XP\n",
       "0         EY          a         a   0.30   0.10   0.16   0.30\n",
       "24        RY          a         a   0.21   0.08   0.10   0.31\n",
       "3         EY          a       the   0.47   0.18   0.44   0.73\n",
       "27        RY          a       the   0.53   0.31   0.30   0.61\n",
       "4         EY      every         a   0.00   0.00   0.00   0.14\n",
       "28        RY      every         a   0.00   0.00   0.00   0.03\n",
       "7         EY      every       the   0.00   0.04   0.01   0.42\n",
       "31        RY      every       the   0.00   0.03   0.00   0.10\n",
       "16        EY       some         a   0.07   0.00   0.00   0.27\n",
       "40        RY       some         a   0.00   0.01   0.00   0.25\n",
       "19        EY       some       the   0.21   0.16   0.13   0.67\n",
       "43        RY       some       the   0.08   0.23   0.10   0.58\n",
       "20        EY        the         a   0.40   0.18   0.16   0.28\n",
       "44        RY        the         a   0.24   0.15   0.17   0.37\n",
       "23        EY        the       the   1.00   1.00   1.00   1.00\n",
       "47        RY        the       the   0.97   0.97   0.97   0.97"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_quant[1\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "81028386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hyohyeongjang/dependency_bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcb8771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
